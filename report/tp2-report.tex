\documentclass[11pt]{article}

\usepackage[a4paper,hmargin=3cm,vmargin=2.6cm]{geometry}
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage{listings}

\usepackage{graphicx}

\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{cleveref}

\bibliography{tp2refs}

%% Set title

\title{\textit{Algorithms for speech and language processing}\\
{\sffamily TP2 Report: \textit{Building a probabilistic parser for French}}}

\author{Wilson Jallet}

\titleformat{\section}[hang]{\LARGE\bfseries\sffamily}{\thesection}{.5em}{}[]
\titleformat{\paragraph}[hang]{\large\bfseries\sffamily}{}{1em}{}[]

%% Set commands

\newcommand{\calN}{\mathcal{N}}
\newcommand{\calO}{\mathcal{O}}

\newcommand{\wer}{\mathrm{WER}}


\lstset{
	basicstyle=\sffamily,
	stringstyle=\sffamily,
	language=Python}

\begin{document}
\maketitle

\section{Implementation}

\paragraph{Building the PCFG and lexicon}

We split the treebank data in training and testing dataset (a 90\%/10\% split), before shuffling the training data and re-splitting between training and validation. We shuffle because lines in the treebank seem to be grouped together thematically\footnote{For instance l. 800--1000 discuss health, and l. 2700 and above seem to discuss French politics.}. For reproducibility of the split, we fixed the value of the random seed.

We use the Python Natural Language ToolKit (NLTK) \cite{nltkCitation} package to parse the treebank data as constituency trees in a way we can extract rules. To avoid sparsity issues, we strip functional tags, e.g. the \lstinline|-SUJ| from tags of the form \lstinline|NP-SUJ|, using a regular expression.
We then strip the lexical rules, leaving the part-of-speech (PoS) as terminals, and build the PCFG from these productions using the data structure provided in NLTK and a helper which computes the empirical probabilities of individual productions.

The lexical rules are transformed into a lexicon using a custom \lstinline|ProbabilisticLexicon| Python class, which holds the triples $(w,\mathrm{PoS}, p(\mathrm{PoS}|w))$ for every PoS and token $w$ (with nonzero probability) stored as a map $w \mapsto \{(\mathrm{PoS},p(\mathrm{PoS}|w))\}$.



\paragraph{Out-of-Vocabulary module}

There are two complementary strategies to propose surrogates for out-of-vocabulary (OOV) words: computing \textbf{spelling} nearest neighbors in the corpus according to the Levenshtein Edit distance, and computing \textbf{semantic} nearest neighbors according to the cosine distance of some embeddings (and intersecting with the corpus vocabulary).

For the Levenshtein-nearest neighbors, we run through the corpus and compute all the distances, and get the $k$ elements with the lowest distance (without sorting).
For the embedding nearest neighbors, we use Scikit-Learn's nearest neighbors implementation \cite{scikit-learn} (which is very efficient), which we fit using the cosine distance to measure semantic similarity\footnote{We actually use the Euclidean distance on normalized embedding vectors, which is equivalent to the cosine distance because $\|\frac{x}{\|x\|} - \frac{y}{\|y\|}\|^2 = 2 - 2\langle \frac{x}{\|x\|}, \frac{y}{\|y\|}\rangle$.}.

We run every input sentence through the module, which keeps in-corpus tokens and selects replacements for OOV words. The OOV module objective is to maximize the sum-total score under a language model trained on the corpus: we use NLTK to construct a unigram-bigram language model with a score function that averages between unigrams and bigrams (called Witten-Bell smoothing). The objective is solved using a greedy strategy: if $w_i$ is OOV, then its replacement $w'_i$ is the proposal that has the best individual score\footnote{Because the language model uses bigrams, it could also be possible to use a dynamic programming strategy and work backwards but this was not investigated.}.

\paragraph{CYK algorithm}

We implement a probabilistic version of the CYK algorithm, as explained on its Wikipedia page \cite{wiki:CYK}. Since the PCFG terminals are parts-of-speech, we integrate the tokens $w$ for the choice of PoS using the lexicon probabilities $p(\mathrm{PoS}|w)$.


\section{Results}


We evaluated our parser on the test dataset. We get the following results: out of 310 sentences, 80 failed to parse, and we obtained an average PoS tag accuracy of $86.6\%$ on the remaining sentences.

Among the errors we noticed, handling OOV proper nouns seems like a source of mistakes, especially when they are the first token in a sentence and thus have no context for the OOV module to use: it happens that common nouns or even verbs are used as replacements when they are picked up by edit distance, see \Cref{fig:npParseFailure_GeorgeRR}.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{georgemartin-fail.png}
	\caption{Failure of parsing a proper noun which is out-of-vocabulary. Here, the names ``George", and ``Martin" are mistakenly identified as \lstinline|NC| and \lstinline|ADJ| because the OOV module replaced them by ``gorge" and ``marin". Replacing the English spelling of the proper noun ``George" by its French spelling ``Georges" produces a correct parse for that token (see \cref{fig:ParseWin_GeorgeRR}).}
	\label{fig:npParseFailure_GeorgeRR}
\end{figure}
This specific issue could be tackled with a better replacement strategy in the OOV module instead of the greedy approach we used. There is also the option of directly handling foreign spelling of proper nouns using some kind of translation module.
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{../example-run.png}
	\caption{This time, ``Georges" (French spelling) is parsed correctly as \lstinline|NPP|.}\label{fig:ParseWin_GeorgeRR}
\end{figure}




\printbibliography{}


\end{document}